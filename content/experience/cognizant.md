+++
title = 'Data Engineer - Cognizant Technology Solutions'
description = "Building scalable ETL systems for financial services and real-time analytics"
featured_image = "/images/abstract-blue-background-simple-design-for-your-website-free-vector.jpg"
date = 2020-10-01T00:00:00-00:00
draft = false
+++

### Hyderabad, India | October 2020 – July 2022

My time at Cognizant was where I truly learned the fundamentals of production data engineering. Working with financial services clients meant dealing with strict compliance requirements, massive data volumes, and zero tolerance for errors. Every pipeline had to be bulletproof because downstream systems were making real financial decisions based on our data.

**Financial Data at Scale**

The technical challenges were substantial—processing over 200GB of financial and customer data daily from a mix of cloud and on-premises sources. Using PySpark and Postgres, I built ETL workflows that had to maintain data lineage for audit purposes while meeting aggressive SLA requirements. The complexity came not just from volume, but from the need to harmonize data from legacy mainframe systems with modern cloud applications.

**Real-Time Analytics for Risk Management**

One of the most impactful projects was building streaming pipelines using Kafka and PySpark that powered real-time analytics for risk and operations teams. Financial services can't wait for batch processing—when market conditions change, risk models need fresh data immediately. Achieving 99.9% data completeness while maintaining sub-second latency taught me that streaming systems require a fundamentally different design philosophy than batch processing.

**Collaboration and Business Impact**

Working closely with business teams was eye-opening. I learned that data engineers aren't just building pipelines—we're enabling better decision-making. When I optimized reporting data marts in Postgres, the 20% improvement in metric accuracy translated directly to better risk assessments and more informed trading decisions.

**Modern DevOps Practices**

This role was where I first experienced modern CI/CD practices at scale. Using Jenkins, Docker, and Git for automated pipeline deployments changed how I thought about code quality and reliability. When your pipeline failures can cost millions in trading losses, you quickly learn the value of comprehensive testing and gradual rollouts.

**Quality and Compliance**

Financial services taught me that data quality isn't optional—it's existential. I implemented comprehensive validation frameworks in Python and SQL that caught errors before they reached production systems. The monthly compliance audits meant every data transformation had to be documented, tested, and traceable.

**Foundation for Growth**

This experience laid the groundwork for everything that came after. Learning to work with cross-functional teams, understanding the business impact of technical decisions, and building systems that operate reliably at scale—these skills became the foundation of my approach to data engineering.

The technical stack was solid but not cutting-edge, which was actually valuable. Sometimes the best learning comes from making older technologies work efficiently rather than always reaching for the newest tools.